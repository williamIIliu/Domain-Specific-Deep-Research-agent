import json
import time
import torch
import random
from typing import List, Dict, Optional, Any
from tqdm import tqdm
from pymilvus import MilvusClient
from transformers import AutoModel, AutoTokenizer
import torch.nn.functional as F
from torch import Tensor

# -------------------------- 1. å¤ç”¨ä½ çš„åŸºç¡€é…ç½®ï¼ˆMilvusã€æ¨¡å‹è·¯å¾„ç­‰ï¼‰ --------------------------
# Milvusé…ç½®
DB_NAME = "Finance_Corpus"
COLLECTION_NAME = "Finance_RAG_helper"
PARTITION_NAME = "base"
INDEX_FIELD = "embedding"  # Milvusä¸­å‘é‡å­—æ®µå
MILVUS_CONFIG = {
    "uri": "http://localhost:19530",
    "token": "root:Milvus",
    "db_name": DB_NAME
}

# æ¨¡å‹é…ç½®ï¼ˆå¤ç”¨ä½ çš„Embeddingæ¨¡å‹è·¯å¾„ï¼‰
MODEL_PATHS = {
    "qwen3": "../../pretrain_weights/embedding/qwen3-0_6b",
    "qwen3_finetune": "../../pretrain_weights/embedding/qwen3-0_6b_finetune",
    "bgem3": "../../pretrain_weights/embedding/bge-m3",
    "gte": "../../pretrain_weights/embedding/gte-large-zh"
}
USE_MODEL = "qwen3"  # é€‰æ‹©è¦ä½¿ç”¨çš„Embeddingæ¨¡å‹
MAX_SEQ_LEN = 1024
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æ•°æ®é›†é…ç½®
ORIGINAL_DATA_PATH = "../../datasets/OmniEval-Corpus/all_data_clean_query.jsonl"  # ä½ çš„åŸå§‹æ•°æ®ï¼ˆå«idã€contentsã€queryï¼‰
OUTPUT_INFONCE_PATH = "../../datasets/OmniEval-Corpus/embedding_finetune/infonce_neg.jsonl"  # è¾“å‡ºçš„InfoNCEæ ¼å¼æ•°æ®
NUM_NEGATIVES = 3  # æ¯ä¸ªæ ·æœ¬éœ€è¦çš„éš¾è´Ÿæ ·æœ¬æ•°é‡ï¼ˆä»Milvusæ£€ç´¢ç»“æœä¸­é€‰ï¼‰
RETRIEVE_TOPK = 5  # Milvusæ£€ç´¢æ—¶å–å‰10ä¸ªç›¸ä¼¼ç»“æœï¼ˆä»ä¸­ç­›é€‰éš¾è´Ÿæ ·æœ¬ï¼‰


# -------------------------- 2. å¤ç”¨ä½ çš„å·¥å…·å‡½æ•°ï¼ˆEmbeddingç”Ÿæˆã€æŒ‡ä»¤æ„é€ ï¼‰ --------------------------
def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    """æå–æœ€åæœ‰æ•ˆtokençš„Embeddingï¼ˆå¤ç”¨ä½ çš„å‡½æ•°ï¼‰"""
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[
            torch.arange(batch_size, device=last_hidden_states.device),
            sequence_lengths
        ]


def compute_embeddings(model, tokenizer, text: str) -> List[float]:
    """ç”Ÿæˆå•æ¡æ–‡æœ¬çš„Embeddingï¼ˆé€‚é…å•æ ·æœ¬æ£€ç´¢åœºæ™¯ï¼‰"""
    # æ–‡æœ¬ç¼–ç 
    batch_dict = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=MAX_SEQ_LEN
    ).to(DEVICE)

    # æ¨¡å‹æ¨ç†ï¼ˆç¦ç”¨æ¢¯åº¦ï¼Œæå‡é€Ÿåº¦ï¼‰
    model.eval()
    with torch.no_grad():
        outputs = model(**batch_dict)
        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])
        embeddings = F.normalize(embeddings, p=2, dim=1)  # L2å½’ä¸€åŒ–ï¼Œé€‚é…Milvusçš„IP/L2è·ç¦»

    # è½¬ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆMilvusæ£€ç´¢è¦æ±‚ï¼‰
    return embeddings[0].detach().cpu().numpy().tolist()


def get_detailed_instruct(task_description: str, query: str) -> str:
    """æ„é€ å¸¦æŒ‡ä»¤çš„queryï¼ˆå¤ç”¨ä½ çš„å‡½æ•°ï¼Œæå‡æ£€ç´¢ç›¸å…³æ€§ï¼‰"""
    return f'æŒ‡ä»¤: {task_description}\næŸ¥è¯¢: {query}'


# -------------------------- 3. æ ¸å¿ƒå‡½æ•°ï¼šåŠ è½½åŸå§‹æ•°æ® + Milvusæ£€ç´¢éš¾è´Ÿæ ·æœ¬ + æ ¼å¼ç»„è£… --------------------------
def load_original_data(file_path: str) -> List[Dict]:
    """åŠ è½½åŸå§‹æ•°æ®ï¼ˆç­›é€‰å«idã€contentsã€queryçš„æœ‰æ•ˆæ ·æœ¬ï¼‰"""
    valid_data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                sample = json.loads(line)
                # å¿…é¡»åŒ…å«æ ¸å¿ƒå­—æ®µï¼šidï¼ˆåŒ¹é…Milvusä¸­çš„idï¼‰ã€contentsï¼ˆæ­£æ ·æœ¬ï¼‰ã€queryï¼ˆé”šç‚¹ï¼‰
                if all(key in sample for key in ["id", "contents", "query"]):
                    # ç®€å•æ¸…æ´—å†…å®¹ï¼ˆé¿å…ç‰¹æ®Šå­—ç¬¦å¯¼è‡´JSONè§£æé”™è¯¯ï¼‰
                    sample["contents"] = sample["contents"].strip().replace("\n", " ").replace("\r", "")
                    sample["query"] = sample["query"].strip()
                    valid_data.append(sample)
                else:
                    print(f"âš ï¸ è·³è¿‡ç¬¬{line_num}è¡Œï¼šç¼ºå°‘id/contents/queryå­—æ®µ")
            except json.JSONDecodeError as e:
                print(f"âš ï¸ è·³è¿‡ç¬¬{line_num}è¡Œï¼šJSONè§£æé”™è¯¯ - {str(e)[:50]}")

    print(f"âœ… æˆåŠŸåŠ è½½ {len(valid_data)} æ¡æœ‰æ•ˆåŸå§‹æ ·æœ¬")
    return valid_data


def retrieve_hard_negatives(
        milvus_client,
        query_embedding: List[float],
        current_sample_id: str,
        topk: int = 10,
        num_samples: int = 1,
        num_filter_samples: int = 3,
) -> List[str]:
    """
    ä» Milvus æ£€ç´¢éš¾è´Ÿæ ·æœ¬ï¼ˆç›¸ä¼¼ä½†éè‡ªèº«çš„æ–‡æ¡£å†…å®¹ï¼‰ï¼Œè¿‡æ»¤å‰è‹¥å¹²æ¡ç»“æœï¼Œåªé‡‡æ ·æŒ‡å®šæ•°é‡ã€‚

    :param milvus_client: Milvuså®¢æˆ·ç«¯å®ä¾‹
    :param query_embedding: å½“å‰æ ·æœ¬ query çš„ Embedding
    :param current_sample_id: å½“å‰æ ·æœ¬çš„ idï¼ˆç”¨äºæ’é™¤è‡ªèº«ï¼‰
    :param topk: æ£€ç´¢çš„æ€»è¿”å›æ•°é‡ï¼ˆå»ºè®® > num_filter_samples + num_samplesï¼‰
    :param num_samples: è¦ä¿ç•™çš„éš¾è´Ÿæ ·æœ¬æ•°é‡ï¼ˆæœ€ç»ˆè¾“å‡ºæ•°é‡ï¼‰
    :param num_filter_samples: è·³è¿‡å‰å¤šå°‘æ¡æœ€ç›¸ä¼¼æ ·æœ¬ï¼ˆé¿å…è‡ªèº«æˆ–è¿‡è¿‘æ ·æœ¬ï¼‰
    :return: éš¾è´Ÿæ ·æœ¬ contents åˆ—è¡¨
    """
    try:
        # è°ƒç”¨ Milvus æ£€ç´¢
        search_result = milvus_client.search(
            collection_name=COLLECTION_NAME,
            partition_names=[PARTITION_NAME],
            anns_field=INDEX_FIELD,
            data=[query_embedding],
            limit=topk,
            output_fields=["id", "text_chunk"],
        )

        hard_candidates = []
        for hit in search_result[0]:
            retrieved_id = str(hit["entity"]["id"])
            retrieved_text = hit["entity"]["text_chunk"].strip()

            # è¿‡æ»¤è‡ªèº«ä¸ç©ºæ–‡æœ¬
            if retrieved_id != current_sample_id and retrieved_text:
                hard_candidates.append(retrieved_text)

        # å¦‚æœå¯ç”¨å€™é€‰ä¸è¶³ï¼Œåˆ™è¿”å›ç©ºï¼ˆä¸å¡«å……ï¼‰
        if len(hard_candidates) <= num_filter_samples:
            print(f"âš ï¸ æœ‰æ•ˆå€™é€‰ä¸è¶³ï¼ˆå…± {len(hard_candidates)} æ¡ï¼Œè¿‡æ»¤ {num_filter_samples} æ¡ï¼‰ -> è¿”å›ç©º")
            return []

        # è¿‡æ»¤æ‰æœ€ç›¸ä¼¼çš„å‰ num_filter_samples æ¡
        filtered_candidates = hard_candidates[num_filter_samples:]

        # å¦‚æœå‰©ä½™æ•°é‡ä¸è¶³æ‰€éœ€é‡‡æ ·æ•°ï¼Œåˆ™ç›´æ¥å…¨éƒ¨è¿”å›ï¼ˆä¸é‡å¤ï¼‰
        if len(filtered_candidates) < num_samples:
            print(f"âš ï¸ éš¾è´Ÿæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œä»…è¿”å› {len(filtered_candidates)} æ¡")
            return filtered_candidates

        # éšæœºé€‰å–æŒ‡å®šæ•°é‡
        selected = random.sample(filtered_candidates, num_samples)
        return selected

    except Exception as e:
        print(f"âŒ æ£€ç´¢éš¾è´Ÿæ ·æœ¬å¤±è´¥ï¼š{str(e)[:120]}")
        return []

def sample_easy_negatives(
    original_data: List[Dict[str, Any]],
    current_sample_id: str,
    current_content: Any,
    num_negatives: int
) -> List[Any]:
    """
    ä»æ•°æ®é›†ä¸­éšæœºé‡‡æ ·ç®€å•è´Ÿæ ·æœ¬ï¼ˆSimple Negativesï¼‰
    è¦æ±‚ï¼š
      1. ä¸èƒ½æ˜¯å½“å‰æ ·æœ¬è‡ªèº«ï¼›
      2. contents çš„ç±»å‹å¿…é¡»ä¸å½“å‰æ ·æœ¬ç›¸åŒï¼›
      3. è´Ÿæ ·æœ¬ä¹‹é—´ä¸é‡å¤ã€‚

    :param original_data: å…¨éƒ¨æ ·æœ¬æ•°æ®ï¼ˆåˆ—è¡¨ï¼‰
    :param current_sample_id: å½“å‰æ ·æœ¬IDï¼ˆç”¨äºæ’é™¤è‡ªèº«ï¼‰
    :param current_content: å½“å‰æ ·æœ¬çš„ contents å€¼ï¼ˆç”¨äºåŒ¹é…ç±»å‹ï¼‰
    :param num_negatives: è¦é‡‡æ ·çš„è´Ÿæ ·æœ¬æ•°é‡
    :return: éšæœºè´Ÿæ ·æœ¬çš„ contents åˆ—è¡¨
    """
    # ç¡®å®šå½“å‰æ ·æœ¬çš„å†…å®¹ç±»å‹
    target_type = type(current_content)

    # æ„é€ å€™é€‰é›†ï¼šæ’é™¤è‡ªèº« + ç±»å‹åŒ¹é…
    candidates = [
        sample["contents"] for sample in original_data
        if sample["id"] != current_sample_id and isinstance(sample["contents"], target_type)
    ]

    # å»é‡ï¼ˆé˜²æ­¢é‡å¤å†…å®¹ï¼‰
    candidates = list(set(map(str, candidates)))  # å…ˆç”¨ str å»é‡ï¼Œå†è½¬å›
    candidates = [json.loads(c) if c.startswith("{") or c.startswith("[") else c for c in candidates]

    # å€™é€‰ä¸è¶³æ—¶ç›´æ¥å…¨éƒ¨ä½¿ç”¨
    if len(candidates) <= num_negatives:
        return random.sample(candidates, len(candidates))

    # å¦åˆ™éšæœºé‡‡æ · num_negatives ä¸ª
    return random.sample(candidates, num_negatives)

def build_infonce_dataset(
        original_data: List[Dict],
        milvus_client: MilvusClient,
        model,
        tokenizer
) -> None:
    """
    æ„å»ºInfoNCEæ ¼å¼æ•°æ®é›†ï¼ˆé”šç‚¹+æ­£æ ·æœ¬+Milvusæ£€ç´¢çš„éš¾è´Ÿæ ·æœ¬ï¼‰
    è¾“å‡ºJSONLæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªInfoNCEæ ·æœ¬
    """
    # æ‰“å¼€è¾“å‡ºæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼Œæ”¯æŒä¸­æ–­åç»§ç»­ç”Ÿæˆï¼‰
    with open(OUTPUT_INFONCE_PATH, "a", encoding="utf-8") as f:
        # è¿›åº¦æ¡æ˜¾ç¤º
        for sample in tqdm(original_data, desc="ğŸ”¨ æ„å»ºInfoNCEæ•°æ®é›†"):
            try:
                # 1. æå–å½“å‰æ ·æœ¬çš„æ ¸å¿ƒä¿¡æ¯
                current_id = str(sample["id"])
                anchor_query = sample["query"]  # é”šç‚¹ï¼šåŸå§‹æ ·æœ¬çš„query
                positive_content = sample["contents"]  # æ­£æ ·æœ¬ï¼šåŸå§‹æ ·æœ¬çš„contents

                # 2. ç”Ÿæˆå¸¦æŒ‡ä»¤çš„query Embeddingï¼ˆæå‡æ£€ç´¢ç›¸å…³æ€§ï¼‰
                task_desc = "æ£€ç´¢ä¸å½“å‰æŸ¥è¯¢ç›¸ä¼¼ä½†ä¸ç›¸å…³çš„é‡‘èæ–‡æ¡£ï¼Œç”¨äºå¯¹æ¯”å­¦ä¹ è®­ç»ƒ"
                instructed_query = get_detailed_instruct(task_desc, anchor_query)
                query_embedding = compute_embeddings(model, tokenizer, instructed_query)

                # 3. ç”¨Milvusæ£€ç´¢éš¾è´Ÿæ ·æœ¬
                hard_negatives = retrieve_hard_negatives(
                    milvus_client=milvus_client,
                    query_embedding=query_embedding,
                    current_sample_id=current_id,
                    topk=5,
                    num_samples=2,  # åªå–ä¸€ä¸ª
                    num_filter_samples=3,  # è·³è¿‡å‰3ä¸ª
                )

                # 4. éšæœºç®€å•è´Ÿæ ·æœ¬ï¼ˆæ•°é‡å¯è‡ªå®šä¹‰ï¼Œæ¯”å¦‚ 2 ä¸ªï¼‰
                simple_negatives = sample_easy_negatives(
                    original_data=original_data,
                    current_sample_id=current_id,
                    current_content=positive_content,
                    num_negatives=2
                )

                # 4. ç»„è£…InfoNCEæ ¼å¼ï¼ˆä¸¥æ ¼ç¬¦åˆä¹‹å‰å®šä¹‰çš„ç»“æ„ï¼‰
                # åˆå¹¶
                if not hard_negatives:
                    all_negatives = list(dict.fromkeys(hard_negatives + simple_negatives))
                else:
                    all_negatives = simple_negatives
                infonce_sample = {
                    "messages": [{"role": "user", "content": anchor_query}],  # é”šç‚¹ï¼ˆqueryï¼‰
                    "positive_messages": [  # æ­£æ ·æœ¬ï¼ˆå½“å‰æ ·æœ¬çš„contentsï¼‰
                        [{"role": "user", "content": positive_content}]
                    ],
                    "negative_messages": [  # éš¾è´Ÿæ ·æœ¬ï¼ˆMilvusæ£€ç´¢åˆ°çš„ç›¸ä¼¼æ–‡æ¡£ï¼‰
                        [{"role": "user", "content": neg_content}] for neg_content in all_negatives
                    ]
                }

                # 5. å†™å…¥è¾“å‡ºæ–‡ä»¶ï¼ˆJSONLæ ¼å¼ï¼‰
                json.dump(infonce_sample, f, ensure_ascii=False)
                f.write("\n")

            except Exception as e:
                # å•ä¸ªæ ·æœ¬å¤„ç†å¤±è´¥ï¼Œè®°å½•æ—¥å¿—å¹¶è·³è¿‡ï¼ˆä¸ä¸­æ–­æ•´ä½“æµç¨‹ï¼‰
                error_msg = f"æ ·æœ¬{current_id[:8]}...å¤„ç†å¤±è´¥ï¼š{str(e)[:100]}"
                print(f"âš ï¸ {error_msg}")
                # å¯é€‰ï¼šå°†å¤±è´¥æ ·æœ¬è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                with open("dataset_build_error.log", "a", encoding="utf-8") as err_f:
                    err_f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} | {error_msg}\n")

    print(f"\nğŸ‰ InfoNCEæ•°æ®é›†æ„å»ºå®Œæˆï¼è¾“å‡ºè·¯å¾„ï¼š{OUTPUT_INFONCE_PATH}")
    # ç»Ÿè®¡æˆåŠŸç”Ÿæˆçš„æ ·æœ¬æ•°
    with open(OUTPUT_INFONCE_PATH, "r", encoding="utf-8") as f:
        success_count = len([line for line in f if line.strip()])
    print(f"ğŸ“Š æˆåŠŸç”Ÿæˆ {success_count} æ¡InfoNCEæ ·æœ¬ï¼ˆæ¯æ¡å«{NUM_NEGATIVES}ä¸ªéš¾è´Ÿæ ·æœ¬ï¼‰")


# -------------------------- 4. ä¸»å‡½æ•°ï¼šä¸²è”æ‰€æœ‰æµç¨‹ --------------------------
def main():
    try:
        print("=" * 80)
        print("ğŸš€ å¼€å§‹åŸºäºMilvusçš„InfoNCEæ•°æ®é›†æ„å»ºæµç¨‹")
        print("=" * 80)

        # æ­¥éª¤1ï¼šåˆå§‹åŒ–Milvuså®¢æˆ·ç«¯ï¼ˆå¤ç”¨ä½ çš„é…ç½®ï¼‰
        print("\n1. åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯")
        milvus_client = MilvusClient(**MILVUS_CONFIG)
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ä¸”å·²åŠ è½½
        if not milvus_client.has_collection(collection_name=COLLECTION_NAME):
            raise ValueError(f"Milvusé›†åˆ {COLLECTION_NAME} ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºå¹¶æ’å…¥æ•°æ®")
        # åŠ è½½é›†åˆï¼ˆè‹¥æœªåŠ è½½ï¼‰
        # if not milvus_client.get_collection_load_state(collection_name=COLLECTION_NAME):
        milvus_client.load_collection(collection_name=COLLECTION_NAME)
        print("âœ… Milvuså®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ˆé›†åˆå·²åŠ è½½ï¼‰")
        # é‡æ–°ç¡®å®šç´¢å¼•
        # 1. IVF-FLAT
        index_params = milvus_client.prepare_index_params()
        index_params.add_index(
            field_name="embedding",
            index_type="IVF_FLAT",
            metric_type="IP",
            params={"nlist": 1024},  # åˆ†æˆå¤šå°‘ç°‡, è¶Šå¤šåˆ’åˆ†è¶Šç»†ï¼Œæ£€ç´¢é€Ÿåº¦æ›´æ…¢ï¼› å‚æ•°ä»32-4096ï¼Œ æˆ‘ä»¬è®¾ç½®128
        )

        # æ­¥éª¤2ï¼šåŠ è½½Embeddingæ¨¡å‹å’ŒTokenizerï¼ˆå¤ç”¨ä½ çš„æ¨¡å‹è·¯å¾„ï¼‰
        print(f"\n2. åŠ è½½Embeddingæ¨¡å‹ï¼š{USE_MODEL}")
        model = AutoModel.from_pretrained(
            pretrained_model_name_or_path=MODEL_PATHS[USE_MODEL],
            local_files_only=True,
            dtype=torch.float16 if DEVICE == "cuda" else torch.float32
        ).to(DEVICE)
        tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path=MODEL_PATHS[USE_MODEL],
            max_length=MAX_SEQ_LEN
        )
        print("âœ… Embeddingæ¨¡å‹å’ŒTokenizeråŠ è½½å®Œæˆ")

        # æ­¥éª¤3ï¼šåŠ è½½åŸå§‹æ•°æ®ï¼ˆå«idã€contentsã€queryï¼‰
        print(f"\n3. åŠ è½½åŸå§‹æ•°æ®ï¼š{ORIGINAL_DATA_PATH}")
        original_data = load_original_data(ORIGINAL_DATA_PATH)
        if not original_data:
            raise ValueError("âŒ æ— æœ‰æ•ˆåŸå§‹æ•°æ®ï¼Œæµç¨‹ç»ˆæ­¢")

        # æ­¥éª¤4ï¼šæ„å»ºInfoNCEæ•°æ®é›†ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰
        print(f"\n4. å¼€å§‹æ„å»ºInfoNCEæ•°æ®é›†ï¼ˆMilvusæ£€ç´¢Top{RETRIEVE_TOPK}ï¼Œé€‰{NUM_NEGATIVES}ä¸ªéš¾è´Ÿæ ·æœ¬ï¼‰")
        build_infonce_dataset(original_data, milvus_client, model, tokenizer)

        print("\n" + "=" * 80)
        print("ğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ æµç¨‹ä¸­æ–­ï¼š{str(e)}")
        traceback.print_exc()
        print("=" * 80)


if __name__ == "__main__":
    import traceback  # å»¶è¿Ÿå¯¼å…¥ï¼Œä»…åœ¨ä¸»å‡½æ•°å¼‚å¸¸æ—¶ä½¿ç”¨

    main()