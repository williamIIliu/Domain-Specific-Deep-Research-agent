"""
æ„å»ºè§’è‰²ç”»åƒçš„ Embedding ç´¢å¼•
ä½¿ç”¨ Qwen3-Embedding æ¨¡å‹å’Œ FAISS è¿›è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æ£€ç´¢
"""
import json
import os
import numpy as np
from typing import List, Dict, Any
from tqdm import tqdm
import faiss
import torch
import torch.nn.functional as F
from torch import Tensor
from modelscope import AutoTokenizer, AutoModel


def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    """è·å–æœ€åä¸€ä¸ª token çš„ hidden state ä½œä¸ºå¥å­è¡¨ç¤º"""
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]


def get_detailed_instruct(task_description: str, query: str) -> str:
    """æ„å»ºæŒ‡ä»¤æ ¼å¼çš„æŸ¥è¯¢"""
    return f'Instruct: {task_description}\nQuery:{query}'


class PersonaIndexBuilder:
    """è§’è‰²ç”»åƒç´¢å¼•æ„å»ºå™¨"""
    
    def __init__(self, model_path: str = "./pretrain_models/embedding/qwen3-0.6b-embedding", 
                 max_length: int = 128, device: str = "auto"):
        """
        åˆå§‹åŒ–ç´¢å¼•æ„å»ºå™¨
        
        Args:
            model_path: Qwen3-Embedding æ¨¡å‹è·¯å¾„
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            device: è®¾å¤‡ç±»å‹
        """
        self.model_path = model_path
        self.max_length = max_length
        self.device = device
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model()
        
        # åˆå§‹åŒ– FAISS ç´¢å¼•
        self.index = None
        self.personas = []
        self.persona_embeddings = []
    
    def _load_model(self):
        """åŠ è½½ Qwen3-Embedding æ¨¡å‹"""
        try:
            print(f"âœ… åŠ è½½ Qwen3-Embedding æ¨¡å‹: {self.model_path}")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                padding_side='left',
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # è®¾ç½®è®¾å¤‡
            if self.device == "auto":
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
            
            self.model.to(self.device)
            self.model.eval()
            
            # è·å– embedding ç»´åº¦
            with torch.no_grad():
                # åˆ›å»ºä¸€ä¸ªæµ‹è¯•è¾“å…¥æ¥è·å–ç»´åº¦
                test_input = self.tokenizer("test", return_tensors="pt", padding=True, truncation=True, max_length=self.max_length)
                test_input.to(self.device)
                outputs = self.model(**test_input)
                test_embedding = last_token_pool(outputs.last_hidden_state, test_input['attention_mask'])
                self.embedding_dim = test_embedding.shape[1]
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œembedding ç»´åº¦: {self.embedding_dim}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def load_personas(self, persona_file: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½è§’è‰²ç”»åƒæ•°æ®
        
        Args:
            persona_file: è§’è‰²ç”»åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            è§’è‰²ç”»åƒåˆ—è¡¨
        """
        personas = []
        print(f"ğŸ“– åŠ è½½è§’è‰²ç”»åƒæ–‡ä»¶: {persona_file}")
        
        with open(persona_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(tqdm(f, desc="è¯»å–è§’è‰²ç”»åƒ")):
                try:
                    # å¤„ç†å•å¼•å·æ ¼å¼çš„ JSON
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·
                    json_str = line.replace("'", '"')
                    persona_data = json.loads(json_str)
                    
                    # æå–è§’è‰²æè¿°
                    persona_text = persona_data.get('persona', '')
                    if persona_text:
                        personas.append({
                            'id': i,
                            'persona': persona_text,
                            'original_data': persona_data
                        })
                        
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ {i+1}: {e}")
                    continue
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(personas)} ä¸ªè§’è‰²ç”»åƒ")
        return personas
    
    def generate_embeddings(self, personas: List[Dict[str, Any]], batch_size: int = 32) -> np.ndarray:
        """
        ä¸ºè§’è‰²ç”»åƒç”Ÿæˆ embeddings
        
        Args:
            personas: è§’è‰²ç”»åƒåˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            embeddings æ•°ç»„
        """
        print(f"ğŸ”„ ç”Ÿæˆ {len(personas)} ä¸ªè§’è‰²ç”»åƒçš„ embeddings...")
        
        # æå–æ–‡æœ¬
        texts = [persona['persona'] for persona in personas]
        
        # æ„å»ºä»»åŠ¡æŒ‡ä»¤
        task = 'Given a persona description, find the most relevant personas for a given query'
        
        all_embeddings = []
        
        with torch.no_grad():
            # æ‰¹é‡å¤„ç†
            for i in tqdm(range(0, len(texts), batch_size), desc="ç”Ÿæˆ embeddings"):
                batch_texts = texts[i:i + batch_size]
                
                # æ„å»ºæŒ‡ä»¤æ ¼å¼çš„è¾“å…¥
                instructed_texts = [
                    get_detailed_instruct(task, text) for text in batch_texts
                ]
                
                # åˆ†è¯
                batch_dict = self.tokenizer(
                    instructed_texts,
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors="pt",
                )
                batch_dict.to(self.device)
                
                # è·å– embeddings
                outputs = self.model(**batch_dict)
                embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])
                
                # å½’ä¸€åŒ–
                embeddings = F.normalize(embeddings, p=2, dim=1)
                
                all_embeddings.append(embeddings.cpu().numpy())
        
        # åˆå¹¶æ‰€æœ‰ embeddings
        final_embeddings = np.vstack(all_embeddings)
        print(f"âœ… ç”Ÿæˆ embeddings å®Œæˆï¼Œå½¢çŠ¶: {final_embeddings.shape}")
        return final_embeddings
    
    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        """
        æ„å»º FAISS ç´¢å¼•
        
        Args:
            embeddings: embeddings æ•°ç»„
            
        Returns:
            FAISS ç´¢å¼•
        """
        print("ğŸ”¨ æ„å»º FAISS ç´¢å¼•...")
        
        # åˆ›å»º FAISS ç´¢å¼• (ä½¿ç”¨ Inner Productï¼Œå› ä¸ºå·²ç»å½’ä¸€åŒ–äº†)
        index = faiss.IndexFlatIP(self.embedding_dim)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        index.add(embeddings.astype('float32'))
        
        print(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {index.ntotal} ä¸ªå‘é‡")
        return index
    
    def save_index(self, index: faiss.Index, personas: List[Dict[str, Any]], 
                   embeddings: np.ndarray, output_dir: str = "./persona_index"):
        """
        ä¿å­˜ç´¢å¼•å’Œç›¸å…³æ•°æ®
        
        Args:
            index: FAISS ç´¢å¼•
            personas: è§’è‰²ç”»åƒåˆ—è¡¨
            embeddings: embeddings æ•°ç»„
            output_dir: è¾“å‡ºç›®å½•
        """
        print(f"ğŸ’¾ ä¿å­˜ç´¢å¼•åˆ°: {output_dir}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ FAISS ç´¢å¼•
        index_path = os.path.join(output_dir, "persona_index.faiss")
        faiss.write_index(index, index_path)
        print(f"âœ… FAISS ç´¢å¼•å·²ä¿å­˜: {index_path}")
        
        # ä¿å­˜è§’è‰²ç”»åƒæ•°æ®
        personas_path = os.path.join(output_dir, "personas.json")
        with open(personas_path, 'w', encoding='utf-8') as f:
            json.dump(personas, f, ensure_ascii=False, indent=2)
        print(f"âœ… è§’è‰²ç”»åƒå·²ä¿å­˜: {personas_path}")
        
        # ä¿å­˜ embeddings (å¯é€‰ï¼Œç”¨äºè°ƒè¯•)
        embeddings_path = os.path.join(output_dir, "persona_embeddings.npy")
        np.save(embeddings_path, embeddings)
        print(f"âœ… Embeddings å·²ä¿å­˜: {embeddings_path}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "model_path": self.model_path,
            "embedding_dim": self.embedding_dim,
            "num_personas": len(personas),
            "index_type": "IndexFlatIP",
            "metric": "cosine_similarity",
            "max_length": self.max_length,
            "device": self.device
        }
        
        metadata_path = os.path.join(output_dir, "metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
        
        print(f"ğŸ‰ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")
    
    def build_index_from_file(self, persona_file: str, output_dir: str = "./persona_index"):
        """
        ä»æ–‡ä»¶æ„å»ºå®Œæ•´çš„ç´¢å¼•
        
        Args:
            persona_file: è§’è‰²ç”»åƒæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        # 1. åŠ è½½è§’è‰²ç”»åƒ
        personas = self.load_personas(persona_file)
        
        # 2. ç”Ÿæˆ embeddings
        embeddings = self.generate_embeddings(personas)
        
        # 3. æ„å»º FAISS ç´¢å¼•
        index = self.build_faiss_index(embeddings)
        
        # 4. ä¿å­˜æ‰€æœ‰æ•°æ®
        self.save_index(index, personas, embeddings, output_dir)
        
        return index, personas, embeddings


class PersonaRetriever:
    """è§’è‰²ç”»åƒæ£€ç´¢å™¨"""
    
    def __init__(self, index_dir: str = "./datasets/persona_index"):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        Args:
            index_dir: ç´¢å¼•ç›®å½•
        """
        self.index_dir = index_dir
        self.load_index()
    
    def load_index(self):
        """åŠ è½½ç´¢å¼•å’Œç›¸å…³æ•°æ®"""
        print(f"ğŸ“‚ ä» {self.index_dir} åŠ è½½ç´¢å¼•...")
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_path = os.path.join(self.index_dir, "metadata.json")
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        # åŠ è½½ FAISS ç´¢å¼•
        index_path = os.path.join(self.index_dir, "persona_index.faiss")
        self.index = faiss.read_index(index_path)
        
        # åŠ è½½è§’è‰²ç”»åƒ
        personas_path = os.path.join(self.index_dir, "personas.json")
        with open(personas_path, 'r', encoding='utf-8') as f:
            self.personas = json.load(f)
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        print(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(self.personas)} ä¸ªè§’è‰²ç”»åƒ")
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            model_path = self.metadata["model_path"]
            max_length = self.metadata.get("max_length", 128)
            device = self.metadata.get("device", "cpu")
            
            print(f"âœ… åŠ è½½æ£€ç´¢æ¨¡å‹: {model_path}")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                padding_side='left',
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModel.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            self.model.to(device)
            self.model.eval()
            
            self.max_length = max_length
            self.device = device
            
            print("âœ… æ£€ç´¢æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def retrieve_similar_personas(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸ä¼¼çš„è§’è‰²ç”»åƒ
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
            
        Returns:
            ç›¸ä¼¼è§’è‰²ç”»åƒåˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢ embedding
        query_embedding = self._generate_query_embedding(query_text)
        
        # æœç´¢
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        # æ„å»ºç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1:  # æœ‰æ•ˆç´¢å¼•
                persona = self.personas[idx].copy()
                persona['similarity_score'] = float(score)
                results.append(persona)
        
        return results
    
    def _generate_query_embedding(self, query_text: str) -> np.ndarray:
        """ç”ŸæˆæŸ¥è¯¢ embedding"""
        try:
            with torch.no_grad():
                # æ„å»ºä»»åŠ¡æŒ‡ä»¤
                task = 'Given a persona description, find the most relevant personas for a given query'
                instructed_query = get_detailed_instruct(task, query_text)
                
                # åˆ†è¯
                batch_dict = self.tokenizer(
                    [instructed_query],
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors="pt",
                )
                batch_dict.to(self.device)
                
                # è·å– embedding
                outputs = self.model(**batch_dict)
                embedding = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])
                
                # å½’ä¸€åŒ–
                embedding = F.normalize(embedding, p=2, dim=1)
                
                return embedding.cpu().numpy()
                
        except Exception as e:
            print(f"âš ï¸ ç”ŸæˆæŸ¥è¯¢ embedding å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
            return np.zeros((1, self.metadata.get("embedding_dim", 1024)))


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ–‡ä»¶è·¯å¾„
    persona_file = "./datasets/persona-hub/finance_persona.jsonl"
    output_dir = "./datasets/persona-hub/finance_persona_index"
    
    # æ„å»ºç´¢å¼• - ä½¿ç”¨ Qwen3-Embedding
    builder = PersonaIndexBuilder(
        model_path="./pretrain_models/embedding/Qwen3-Embedding-0.6B",
        max_length=128,
        device="auto"
    )

    print("ğŸš€ å¼€å§‹æ„å»ºè§’è‰²ç”»åƒç´¢å¼•...")
    
    try:
        index, personas, embeddings = builder.build_index_from_file(
            persona_file=persona_file,
            output_dir=output_dir
        )
        
        print("\nğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        
        # æµ‹è¯•æ£€ç´¢
        retriever = PersonaRetriever(output_dir)
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "è‚¡ç¥¨æŠ•èµ„å’ŒåŸºé‡‘ç†è´¢",
            "å°é¢ä¿¡è´·å’Œåˆ›ä¸šæ”¯æŒ", 
            "é£é™©ç®¡ç†ä¸“å®¶",
            "æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ "
        ]
        
        for query in test_queries:
            print(f"\nğŸ“ æŸ¥è¯¢: {query}")
            results = retriever.retrieve_similar_personas(query, top_k=3)
            
            for i, result in enumerate(results, 1):
                print(f"  {i}. [{result['similarity_score']:.3f}] {result['persona']}")
        
        print("\nâœ… ç´¢å¼•æ„å»ºå’Œæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ„å»ºç´¢å¼•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()