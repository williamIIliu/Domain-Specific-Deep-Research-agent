import os
import json
import time
from typing import Any, Dict, List
from rank_bm25 import BM25Okapi

import numpy as np
import torch
from pymilvus import (
    MilvusClient,
    FieldSchema,
    CollectionSchema,
    DataType,
    Function,
    FunctionType,
)

# ==========================
# 1ï¸âƒ£ è¿æ¥ Milvus
# ==========================
DB_NAME = "Finance_Corpus"
COLLECTION_NAME = "Finance_RAG_helper_hybrid"
PARTITION_NAME = "base"
DATA_PATH = "../../datasets/OmniEval-Corpus/demo/test_embedding.jsonl"

root_client = MilvusClient(
    uri="http://localhost:19530",
    user="root",
    password="Milvus",
)

# æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
db_list = root_client.list_databases()
if DB_NAME in db_list:
    print(f"ğŸ” æ£€æŸ¥æ•°æ®åº“ {DB_NAME} ...")
    # âš ï¸ å…ˆè¿æ¥åˆ°è¯¥æ•°æ®åº“ï¼Œåˆ é™¤æ‰€æœ‰ collection
    temp_client = MilvusClient(
        uri="http://localhost:19530",
        user="root",
        password="Milvus",
        db_name=DB_NAME,
    )
    collections = temp_client.list_collections()
    for c in collections:
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§ collection: {c}")
        temp_client.drop_collection(c)

    # ç„¶åå†å®‰å…¨åˆ é™¤æ•°æ®åº“
    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ•°æ®åº“: {DB_NAME}")
    root_client.drop_database(DB_NAME)

# é‡æ–°åˆ›å»ºæ•°æ®åº“
print(f"âœ… åˆ›å»ºæ–°æ•°æ®åº“: {DB_NAME}")
root_client.create_database(DB_NAME)

# è¿æ¥åˆ°æ–°æ•°æ®åº“
client = MilvusClient(
    uri="http://localhost:19530",
    user="root",
    password="Milvus",
    db_name=DB_NAME,
)
# ==========================
# 2ï¸âƒ£ å®šä¹‰ Schema
# ==========================
analyzer_params = {"type": "chinese"}  # ä¸­æ–‡åˆ†è¯å™¨

fields = [
    FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=36, is_primary=True),
    FieldSchema(
        name="text_chunk",
        dtype=DataType.VARCHAR,
        max_length=1024,
        enable_analyzer=True,
        analyzer_params=analyzer_params,
        enable_match=True,
    ),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1024),
    FieldSchema(name="sparse_bm25", dtype=DataType.SPARSE_FLOAT_VECTOR),
    FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=256),
    FieldSchema(name="publish_time", dtype=DataType.INT64),
    FieldSchema(name="metadata", dtype=DataType.JSON, enable_dynamic=True),
]

schema = CollectionSchema(fields,
                          enable_dynamic_field=True,
                          description="Finance RAG helper collection")

bm25_function = Function(
    name="bm25_fn",
    input_field_names=["text_chunk"],
    output_field_names="sparse_bm25",
    function_type=FunctionType.BM25,
)

schema.add_function(bm25_function)

# ==========================
# 3ï¸âƒ£ å®šä¹‰ç´¢å¼•
# ==========================
index_params = MilvusClient.prepare_index_params()

index_params.add_index(
    field_name="embedding",
    index_name="embedding_index",
    index_type="IVF_FLAT",
    metric_type="IP",
    params={"nlist": 1024},
)

index_params.add_index(
    field_name="sparse_bm25",
    index_name="sparse_bm25_index",
    index_type="sparse_inverted_index",
    metric_type="BM25",
    params={"inverted_index_algo": "DAAT_MAXSCORE"}, # Algorithm used for building and querying the index
)


# ==========================
# 4ï¸âƒ£ åˆ›å»º Collection å’Œåˆ†åŒº
# ==========================
collections = client.list_collections()
if COLLECTION_NAME in collections:
    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„ collection: {COLLECTION_NAME}")
    client.drop_collection(COLLECTION_NAME)

print(f"ğŸ“¦ åˆ›å»ºæ–°çš„ collection: {COLLECTION_NAME}")
collection = client.create_collection(
    collection_name=COLLECTION_NAME,
    schema=schema,
)

# åˆ›å»ºåˆ†åŒº
# partitions = client.list_partitions(COLLECTION_NAME)
# if PARTITION_NAME not in partitions:
#     client.create_partition(COLLECTION_NAME, PARTITION_NAME)
#     print(f"ğŸ“‚ åˆ›å»ºåˆ†åŒº: {PARTITION_NAME}")

# ==========================
# 5ï¸âƒ£ æ’å…¥æ•°æ®
# ==========================
dataset = []
print(f"ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {DATA_PATH}")

import jieba
# è¯»å–æ•°æ®
records = []
texts = []
with open(DATA_PATH, "r", encoding="utf-8") as f:
    for line in f:
        data = json.loads(line)
        records.append(data)
        tokens = list(jieba.cut(data["text_chunk"]))
        texts.append(tokens)

# Step 1: å¯¹æ–‡æœ¬åšä¸­æ–‡åˆ†è¯
tokenized_texts = [list(jieba.cut(d["text_chunk"])) for d in dataset]

# Step 2: æ„å»º BM25 æ¨¡å‹
bm25 = BM25Okapi(tokenized_texts)

# Step 3: æ„å»ºå…¨è¯è¡¨
vocab = list(set(token for doc in tokenized_texts for token in doc))
vocab_index = {word: i for i, word in enumerate(vocab)}

# Step 4: ä¸ºæ¯æ¡è®°å½•ç”Ÿæˆ sparse BM25 vector
for i, record in enumerate(dataset):
    tokens = tokenized_texts[i]
    scores = bm25.get_scores(tokens)
    sparse_vector = [0.0] * len(vocab)
    for t in tokens:
        idx = vocab_index[t]
        sparse_vector[idx] = float(scores[idx])
    record["sparse_bm25"] = sparse_vector

batch_size = 500
n = len(dataset)
total_time = 0.0
print(f"ğŸ“¦ æ€»è®¡å¾…æ’å…¥æ•°æ®: {n} æ¡")

for i in range(0, n, batch_size):
    batch = dataset[i: i + batch_size]
    start = time.time()
    client.insert(
        collection_name=COLLECTION_NAME,
        data=batch,
        partition_name=PARTITION_NAME,
    )
    elapsed = time.time() - start
    total_time += elapsed
    print(f"âœ… å·²æ’å…¥ {min(i + batch_size, n)} / {n} æ¡ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’")

print(f"ğŸ æ€»è€—æ—¶ {total_time:.2f} ç§’ï¼Œå¹³å‡ {(total_time / n) * 1000:.2f} ms/æ¡")

# ==========================
# âœ… éªŒè¯ Schema
# ==========================
info = client.describe_collection(COLLECTION_NAME)
for field in info["fields"]:
    print(f"å­—æ®µ {field['name']} nullable: {field.get('nullable', False)}")

print("ğŸ‰ Collection åˆ›å»ºä¸æ•°æ®æ’å…¥å…¨éƒ¨å®Œæˆï¼")
