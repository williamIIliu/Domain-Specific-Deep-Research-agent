import os
import json
import numpy as np
import faiss
from tqdm import tqdm
import subprocess

# ------------------------- æ–‡æœ¬è¯»å– -------------------------
def load_corpus(corpus_path: str):
    corpus= []
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue  # è·³è¿‡ç©ºè¡Œ
            try:
                # è§£æå•è¡ŒJSON
                data = json.loads(line)

                # æå–å¿…è¦å­—æ®µ
                item = {
                    "id": data.get("id", ""),  # ç¡®ä¿idå­˜åœ¨ï¼Œæ— åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
                    "contents": "",
                    "embedding": None  # é¢„ç•™Embeddingå­—æ®µï¼Œæš‚ä¸ºNone
                }

                # å¤„ç†contentså­—æ®µ
                contents = data.get("contents", "")
                if isinstance(contents, dict):
                    # è‹¥ä¸ºå­—å…¸ï¼Œè½¬æ¢ä¸º "key":"value" æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰
                    item["contents"] = ", ".join([f'"{k}":"{v}"' for k, v in contents.items()])
                else:
                    # è‹¥ä¸ºå­—ç¬¦ä¸²ï¼Œç›´æ¥ä¿ç•™
                    item["contents"] = str(contents)  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹

                # å¤„ç†embeddingå­—æ®µ
                item["embedding"] = data.get("embedding", "")
                corpus.append(item)

            except json.JSONDecodeError as e:
                print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥ - {str(e)}")
            except Exception as e:
                print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡Œå¤„ç†å¤±è´¥ - {str(e)}")

    return corpus

# ------------------------- FAISS Index -------------------------
def build_faiss_index(jsonl_path, save_path, batch_size=1000):
    """
    ä» JSONL æ•°æ®æ„å»º FAISS å‘é‡ç´¢å¼•å¹¶ä¿å­˜
    """

    def load_embeddings(jsonl_path, batch_size=1000):
        """
        åˆ†æ‰¹åŠ è½½ JSONL æ–‡ä»¶ä¸­çš„ embedding å‘é‡å’Œå¯¹åº” id
        """
        embeddings_batch = []
        ids_batch = []

        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                obj = json.loads(line)
                emb = obj.get("embedding", None)
                if emb is not None:
                    embeddings_batch.append(np.array(emb, dtype=np.float32))
                    ids_batch.append(obj.get("id"))

                # æ‰¹é‡è¿”å›
                if len(embeddings_batch) >= batch_size:
                    yield np.stack(embeddings_batch, axis=0), ids_batch
                    embeddings_batch, ids_batch = [], []

        # è¿”å›å‰©ä½™éƒ¨åˆ†
        if embeddings_batch:
            yield np.stack(embeddings_batch, axis=0), ids_batch

    print(f"ğŸ“‚ æ­£åœ¨ä» {jsonl_path} åŠ è½½å‘é‡å¹¶æ„å»ºç´¢å¼•...")

    index = None
    all_ids = []

    for embeddings, ids in tqdm(load_embeddings(jsonl_path, batch_size=batch_size)):
        if index is None:
            dim = embeddings.shape[1]
            # è¿™é‡Œä½¿ç”¨ç®€å•çš„ L2 è·ç¦»ç´¢å¼•ï¼Œä¹Ÿå¯æ”¹ç”¨ IndexFlatIPï¼ˆå†…ç§¯ç›¸ä¼¼åº¦ï¼‰
            index = faiss.IndexFlatIP(dim)
        index.add(embeddings)
        all_ids.extend(ids)

    if index is None:
        raise ValueError("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½• embedding æ•°æ®ã€‚")

    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(all_ids)} æ¡å‘é‡ã€‚")

    # ä¿å­˜ faiss ç´¢å¼•
    faiss_index_path = os.path.join(save_path, "faiss_index.bin")
    faiss.write_index(index, faiss_index_path)
    print(f"ğŸ’¾ FAISS ç´¢å¼•å·²ä¿å­˜åˆ°: {faiss_index_path}")

    # ä¿å­˜ id å¯¹åº”è¡¨
    id_map_path = os.path.join(save_path, "id_map.json")
    with open(id_map_path, "w", encoding="utf-8") as f:
        json.dump(all_ids, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ID æ˜ å°„è¡¨å·²ä¿å­˜åˆ°: {id_map_path}")
    print("FAISS ä¿å­˜å·¥ä½œå®Œæˆï¼")


# ------------------------- BM25 Index -------------------------
def build_BM25_index(jsonl_path, json_slice_path, index_path):
    # åˆ›å»ºç›®å½•
    os.makedirs(json_slice_path, exist_ok=True)
    os.makedirs(index_path, exist_ok=True)

    # æ‹†åˆ†JSONLä¸ºå•ä¸ªæ–‡ä»¶
    cnt = 0
    with open(jsonl_path, "r", encoding="utf-8") as fin:
        for line in tqdm(fin, desc="æ‹†åˆ†æ–‡ä»¶"):
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            cnt += 1
            docid = obj["id"]
            # æå–æ–‡æœ¬å†…å®¹ï¼Œç¡®ä¿æ•°å­—å­—æ®µä¹Ÿè¢«ç´¢å¼•
            raw_content = obj.get("contents") or obj.get("text_chunk") or obj.get("context") or ""
            
            if isinstance(raw_content, dict):
                # å°†å­—å…¸çš„æ‰€æœ‰é”®å€¼å¯¹è½¬ä¸º "é”®:å€¼" æ ¼å¼ï¼Œæ•°å­—ä¹Ÿè½¬ä¸ºå­—ç¬¦ä¸²
                # ä¾‹å¦‚: {"è‚¡ç¥¨ä»£ç ":"002851","è¡Œä¸šåç§°":"ç”µåŠ›è®¾å¤‡"} 
                # è½¬ä¸º: "è‚¡ç¥¨ä»£ç :002851 è¡Œä¸šåç§°:ç”µåŠ›è®¾å¤‡"
                text = " ".join([f"{k}:{v}" for k, v in raw_content.items() if v is not None])
            elif isinstance(raw_content, list):
                text = json.dumps(raw_content, ensure_ascii=False)
            else:
                text = str(raw_content)
            if not text.strip():
                continue
            # å†™å…¥æ–‡ä»¶
            with open(f"{json_slice_path}/{docid}.json", "w", encoding="utf-8") as fout:
                json.dump({"id": docid, "contents": text}, fout, ensure_ascii=False)

    print(f"æ‹†åˆ†å®Œæˆï¼Œå¤„ç†{cnt}æ¡æ•°æ®")

    # æ„å»ºç´¢å¼•
    subprocess.run([
        "python", "-m", "pyserini.index.lucene",
        "--collection", "JsonCollection",
        "--input", json_slice_path,
        "--index", index_path,
        "--generator", "DefaultLuceneDocumentGenerator",
        "--threads", "8",
        "--storePositions", "--storeDocvectors", "--storeRaw"
    ], check=True)
    print("BM25ç´¢å¼•æ„å»ºå®Œæˆ")


if __name__ == "__main__":
    jsonl_path = "./datasets/database/data_with_embedding_shards/all_data_clean_embedding.jsonl"

    # è¯»å–æ–‡æœ¬æ•°æ®
    datasets = load_corpus(jsonl_path)
    print(len(datasets))
    print(datasets[0])

    # FAISS index ç”Ÿæˆ
    save_path = "./datasets/database/faiss_qwen"  # è¾“å‡ºä¿å­˜ç›®å½•
    batch_size = 1024  # æ‰¹æ¬¡å¤§å°ï¼Œå¯æ ¹æ®å†…å­˜è°ƒæ•´
    os.makedirs(save_path, exist_ok=True)
    build_faiss_index(jsonl_path, save_path, batch_size)

    # BM25 index ç”Ÿæˆ
    json_slice_path = "./datasets/database/bm25_tokenize"
    index_path = "./datasets/database/bm25"
    build_BM25_index(jsonl_path, json_slice_path, index_path)



